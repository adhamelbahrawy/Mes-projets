{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "df = pd.read_csv(\"./predictions/y_pred_DT4.csv\", header=None)\n",
    "temp = df[0]\n",
    "predicted = temp.values\n",
    "\n",
    "\n",
    "ds = pd.read_csv(\"./predictions/y_test.csv\", header=None)\n",
    "tempp = ds[0]\n",
    "actual = tempp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tp(predicted,actual,C):\n",
    "    TP = 0\n",
    "    for i in range(0,len(actual)):\n",
    "        if actual[i] == predicted[i] and actual[i] == C:\n",
    "            TP+=1\n",
    "    #print(\"True Positive of class \",C,\":\" , TP)\n",
    "    return TP \n",
    "    \n",
    "def Fp(predicted,actual,C):  \n",
    "    FP = 0\n",
    "    for i in range(0,len(actual)):\n",
    "        if actual[i] != predicted[i] and predicted[i] == C:\n",
    "           FP+=1\n",
    "    #print(\"False Positive of class \",C,\": \", FP)\n",
    "    return  FP \n",
    "\n",
    "def Tn(predicted,actual,C):\n",
    "    TN = 0\n",
    "    for i in range(0,len(actual)):\n",
    "        if actual[i] != C and actual[i] != C:\n",
    "            TN+=1\n",
    "    #print(\"True Negative of class \",C,\": \", TN)\n",
    "    return TN \n",
    "\n",
    "def Fn(predicted,actual,C):\n",
    "    FN = 0\n",
    "    for i in range(0,len(actual)):\n",
    "        if actual[i] == C and predicted[i] != C:\n",
    "            FN+=1\n",
    "    #print(\"False Negative of class \",C,\": \", FN)\n",
    "    return FN \n",
    "\n",
    "def accuracy(TP,FP,TN,FN):\n",
    "    accuracy = (TP + TN)/(TP + FP + TN + FN)\n",
    "\n",
    "    return accuracy*100\n",
    "\n",
    "def precision(TP,FP):\n",
    "    precision = TP/(TP+FP)\n",
    "    return precision\n",
    "\n",
    "def recall(TP,FN):\n",
    "    if (TP+FN)!= 0 :\n",
    "        recall = (TP)/(TP+FN)\n",
    "        return recall\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "def F1_Score(recall : float ,precision : float):\n",
    "    f1 = 2*(recall * precision )/(recall + precision )\n",
    "    return f1\n",
    "\n",
    "def matricedeconfusion(actual,predicted):\n",
    "    data = {'actual' : actual, 'predicted': predicted}\n",
    "    df = pd.DataFrame(data, columns=['actual','predicted'])\n",
    "    confusion_matrix = pd.crosstab(df['actual'], df['predicted'], rownames=['actual'], colnames=['predicted'])\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Éxactitude de la classe 0: 84.765625 %\n",
      "Éxactitude de la classe 1: 86.98224852071006 %\n",
      "Éxactitude de la classe 2: 84.5213849287169 %\n",
      "Éxactitude de la classe 3: 90.06479481641469 %\n",
      "Precision de la classe 0: 0.7142857142857143\n",
      "Precision de la classe 1: 0.7577319587628866\n",
      "Precision de la classe 2: 0.5866666666666667\n",
      "Precision de la classe 3: 0.6666666666666666\n",
      "Recall de la classe 0: 0.8333333333333334\n",
      "Recall de la classe 1: 0.8855421686746988\n",
      "Recall de la classe 2: 0.4943820224719101\n",
      "Recall de la classe 3: 0.12244897959183673\n",
      "F1 Score de la classe 0: 0.7692307692307692\n",
      "F1 Score de la classe 1: 0.8166666666666668\n",
      "F1 Score de la classe 2: 0.5365853658536585\n",
      "F1 Score de la classe 3: 0.20689655172413793\n",
      "predicted    0    1   2  3\n",
      "actual                    \n",
      "0          130   19   6  1\n",
      "1           15  147   2  2\n",
      "2           25   20  44  0\n",
      "3           12    8  23  6\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#Classification_report\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(actual,predicted))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#TRUE POSITIVE\n",
    "#CLASSE 0 A 4\n",
    "Tpclasse0 = Tp(predicted,actual,0)\n",
    "Tpclasse1 = Tp(predicted,actual,1)\n",
    "Tpclasse2 = Tp(predicted,actual,2)\n",
    "Tpclasse3 = Tp(predicted,actual,3)\n",
    "\n",
    "# FALSE POSITIVE\n",
    "#CLASSE 0 A 4\n",
    "Fpclasse0 = Fp(predicted,actual,0)\n",
    "Fpclasse1 = Fp(predicted,actual,1)\n",
    "Fpclasse2 = Fp(predicted,actual,2)\n",
    "Fpclasse3 = Fp(predicted,actual,3)\n",
    "\n",
    "#TRUE NEGATIVE\n",
    "#CLASSE 0 A 4\n",
    "Tnclasse0 = Tn(predicted,actual,0)\n",
    "Tnclasse1 = Tn(predicted,actual,1)\n",
    "Tnclasse2 = Tn(predicted,actual,2)\n",
    "Tnclasse3 = Tn(predicted,actual,3)\n",
    "\n",
    "#FALSE NEGATIVE\n",
    "#CLASSE 0 A 4\n",
    "Fnclasse0 = Fn(predicted,actual,0)\n",
    "Fnclasse1 = Fn(predicted,actual,1)\n",
    "Fnclasse2 = Fn(predicted,actual,2)\n",
    "Fnclasse3 = Fn(predicted,actual,3)\n",
    "\n",
    "#1 ACCURACY\n",
    "AccuracyClass0 = accuracy(Tpclasse0,Fpclasse0,Tnclasse0,Fnclasse0)\n",
    "AccuracyClass1 = accuracy(Tpclasse1,Fpclasse1,Tnclasse1,Fnclasse1)\n",
    "AccuracyClass2 = accuracy(Tpclasse2,Fpclasse2,Tnclasse2,Fnclasse2)\n",
    "AccuracyClass3 = accuracy(Tpclasse3,Fpclasse3,Tnclasse3,Fnclasse3)\n",
    "print(\"Éxactitude de la classe 0:\", AccuracyClass0,\"%\")\n",
    "print(\"Éxactitude de la classe 1:\", AccuracyClass1,\"%\")\n",
    "print(\"Éxactitude de la classe 2:\", AccuracyClass2,\"%\")\n",
    "print(\"Éxactitude de la classe 3:\", AccuracyClass3,\"%\")\n",
    "#2 PRECISION \n",
    "precisionClasse0 = precision(Tpclasse0,Fpclasse0)\n",
    "precisionClasse1 = precision(Tpclasse1,Fpclasse1)\n",
    "precisionClasse2 = precision(Tpclasse2,Fpclasse2)\n",
    "precisionClasse3 = precision(Tpclasse3,Fpclasse3)\n",
    "print(\"Precision de la classe 0:\", precisionClasse0)\n",
    "print(\"Precision de la classe 1:\", precisionClasse1)\n",
    "print(\"Precision de la classe 2:\", precisionClasse2)\n",
    "print(\"Precision de la classe 3:\", precisionClasse3)\n",
    "\n",
    "\n",
    "#3 RECALL\n",
    "RecallClasse0 = recall(Tpclasse0,Fnclasse0)\n",
    "RecallClasse1 = recall(Tpclasse1,Fnclasse1)\n",
    "RecallClasse2 = recall(Tpclasse2,Fnclasse2)\n",
    "RecallClasse3 = recall(Tpclasse3,Fnclasse3)\n",
    "print(\"Recall de la classe 0:\", RecallClasse0 )\n",
    "print(\"Recall de la classe 1:\", RecallClasse1 )\n",
    "print(\"Recall de la classe 2:\", RecallClasse2 )\n",
    "print(\"Recall de la classe 3:\", RecallClasse3 )\n",
    "#4 F1_Score\n",
    "F1_ScoreClasse0 = F1_Score(RecallClasse0,precisionClasse0)\n",
    "F1_ScoreClasse1 = F1_Score(RecallClasse1,precisionClasse1)\n",
    "F1_ScoreClasse2 = F1_Score(RecallClasse2,precisionClasse2)\n",
    "F1_ScoreClasse3 = F1_Score(RecallClasse3,precisionClasse3)\n",
    "print(\"F1 Score de la classe 0:\", F1_ScoreClasse0 )\n",
    "print(\"F1 Score de la classe 1:\", F1_ScoreClasse1 )\n",
    "print(\"F1 Score de la classe 2:\", F1_ScoreClasse2 )\n",
    "print(\"F1 Score de la classe 3:\", F1_ScoreClasse3 )\n",
    "\n",
    "#Matrice de confusion\n",
    "x = matricedeconfusion(actual,predicted)\n",
    "print(x)\n",
    "#Classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(actual,predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
